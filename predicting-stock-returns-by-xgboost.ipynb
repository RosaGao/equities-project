{"cells":[{"cell_type":"markdown","metadata":{},"source":["We are going to use XGBoost regressor to predict the stock returns in the following two days. I did not see much notebook here. Let me know if it is unproper to post this note.\n","\n","The key ideas in this work are  \n","(1) using XGBoost to preselect the ten most important features to fight overfitting,  \n","(2) using the linear loss function instead of the conventional square loss function.\n","\n","The final model consistently outperforms the all-zero predict. In my test, the model also performed better than the median prediction and could rank 30th in the private leaderboard."]},{"cell_type":"markdown","metadata":{},"source":["# Data Visualization and Feature Analysis"]},{"cell_type":"markdown","metadata":{},"source":["We load and visualize the data in the section. We also analyze the correlations between the features.\n","\n","To load the zipped data, we use the zipfile package. The training data are loaded into a DataFrame in Pandas."]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'tensorflow'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/var/folders/b0/9jm5rsy91291mltfrk5tqbpr0000gn/T/ipykernel_8250/776036856.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mzipfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"]}],"source":["def mean_before_date(df, feature):\n","  col = df[feature]\n","  "]},{"cell_type":"markdown","metadata":{},"source":["The testing data are also loaded to a DataFrame in Pandas."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["zip_file = ZipFile('../input/the-winton-stock-market-challenge/test_2.csv.zip')\n","new_df = pd.read_csv(zip_file.open('test_2.csv'))\n","new_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["Most of the columns contain missing values. Below are the top ten features with the most missing values."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(figsize = (15, 5))\n","df_na = (df.isnull().sum() / len(df))\n","df_na = df_na.drop(df_na[df_na == 0].index).sort_values(ascending = False)[: 10]\n","ax.bar(range(df_na.size), df_na, width = 0.5)\n","plt.xticks(range(df_na.size), df_na.index, rotation = 0)\n","plt.ylim([0, 1])\n","plt.title('Top ten features with the most missing values')\n","plt.ylabel('Missing ratio')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We separate the time series of the stock returns in every minute from the independent variables. I tried to use the Recurrent Neural Network (RNN) to deal with the time series, but the RNN suffers from overfitting very badly. I finally decide to take the sum of all returns in the time series as a single feature representing the stock return in the first two hours in the current day. \n","\n","Similarly, I tried many models to predict the time series of the intra-day stock returns. None of them beat the all-zero prediction. This is reasonable because in real life we only need to predict the stock return in the next minute. We can update our model every minute with new observations. Therefore, I will just use the all-zero prediction for the intra-day stock returns and focus on predicting the stock returns in the two following days."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ret = df.loc[:, 'Ret_2':'Ret_120'].sum(1)\n","new_ret = new_df.loc[:, 'Ret_2':'Ret_120'].sum(1)\n","\n","X = np.hstack((df.loc[:, 'Feature_1':'Ret_MinusOne'].values, ret.values[:, np.newaxis]))\n","ts = df.loc[:, 'Ret_2':'Ret_120'].values\n","y = df.loc[:, 'Ret_PlusOne':'Ret_PlusTwo'].values\n","y_ts = df.loc[:, 'Ret_121':'Ret_180'].values\n","\n","new_X = np.hstack((new_df.loc[:, 'Feature_1':'Ret_MinusOne'].values, new_ret.values[:, np.newaxis]))\n","new_ts = new_df.loc[:, 'Ret_2':'Ret_120'].values"]},{"cell_type":"markdown","metadata":{},"source":["Since we do not know what those independent variables stand for, we just impute the missing values with the mean of that feature. If we are in a real world knowing the meaning of those independent variables, we may find better imputation methods."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.impute import SimpleImputer\n","\n","imr = SimpleImputer(strategy = 'mean')\n","X = imr.fit_transform(X)\n","new_X = imr.transform(new_X)\n","\n","ts = imr.fit_transform(ts)\n","new_ts = imr.transform(new_ts)"]},{"cell_type":"markdown","metadata":{},"source":["Below are three examples of the time series of the stock returns in the first 120 minutes of the current day. We see that different stock can behaves very different. Detailed analysis of the time series will give us much information on the volatilities of the stocks, which are very useful in pricing options on the stocks.\n","\n","However, the details of the time series are too noisy to be useful to predict the average stock returns. As I mentioned before, we take the sum of the returns in the time series as a single feature."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(figsize = (10, 5))\n","plt.plot(ts[0, :], label = 'Stock 1')\n","plt.plot(ts[1, :], label = 'Stock 2')\n","plt.plot(ts[2, :], label = 'Stock 3')\n","plt.ylim([-0.004, 0.004])\n","plt.xlim([0, 118])\n","plt.ylabel('Stock returns')\n","plt.xlabel('Minutes')\n","plt.title('Examples of the stock returns in the first 120 minutes of the current day')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We draw the correlations between different features in the data. Some independent variables are strongly correlated with each other. We will discuss this in the following two cells. We also notice that the target variables are almost not correlated with the independent features. This implies that it is very hard for our model to beat some naive predictions such as the all-zero prediction and the median prediction."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from mlxtend.plotting import heatmap\n","\n","cm = np.corrcoef(np.hstack([X, y]).T)\n","cols = list(df.columns[1:28]) + ['Ret', 'Ret_PlusOne', 'Ret_PlusTwo']\n","hm = heatmap(cm, row_names = cols, column_names = cols, figsize = (20, 20))\n","plt.title('Correlations Between the Different Features of the Data', fontsize = 20)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Here is an example of a strong correlation between two independent variables."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.scatter(df['Feature_3'], df['Feature_11'], marker = 'o', s = 1) #, linewidth = 1, edgecolor = 'black')\n","plt.title('Correlation between Feature_3 and Feature_11')\n","plt.xlabel('Feature_3')\n","plt.ylabel('Feature_11')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We can use the Principal Component Analysis (PCA) to decouple those independent variables. Below is an example of using PCA. However, my experience shows that using PCA make our model perform worse here. I think this is because many of the independent variable are not useful in predicting future stock returns. In using PCA, we mix useful independent variables and useless independent variables, which make our model perform worse. Therefore, we will not use PCA here."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#from sklearn.preprocessing import StandardScaler\n","#from sklearn.decomposition import PCA\n","\n","#scaler = StandardScaler()\n","#X[:, :25] = scaler.fit_transform(X[:, :25])\n","#pca = PCA(n_components = 25)\n","#X[:, :25] = pca.fit_transform(X[:, :25])"]},{"cell_type":"markdown","metadata":{},"source":["The data are split into training data, validation data, and testing data. From the discuss section of this Challenge, we know the Feature_7 is closely related to the time. Therefore, we do not use cross-validation or random splitting here. Otherwise, future information will leak into the training data, and we will get a model that performs very good on the training data but is not useful in predicting future unseen data."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_train = X[:30000, :]\n","X_val = X[30000:35000, :]\n","X_test = X[35000:, :]\n","y_train = y[:30000, :]\n","y_val = y[30000:35000, :]\n","y_test = y[35000:, :]\n","X_train_val = X[:35000, :]\n","y_train_val = y[:35000, :]"]},{"cell_type":"markdown","metadata":{},"source":["The stock returns in the following two days are split to two target variables."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y1_train = y_train[:, 0]\n","y2_train = y_train[:, 1]\n","y1_val = y_val[:, 0]\n","y2_val = y_val[:, 1]\n","y1_test = y_test[:, 0]\n","y2_test = y_test[:, 1]\n","y1_train_val = y_train_val[:, 0]\n","y2_train_val = y_train_val[:, 1]\n","y1 = y[:, 0]\n","y2 = y[:, 1]"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Selection with XGBoost"]},{"cell_type":"markdown","metadata":{},"source":["We use a preliminary XGBoost regressor to do feature selections in this section.\n","\n","The Numpy ndarray are transformed into a DMatrix to be compatible with the xgboost package."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import xgboost as xgb\n","\n","dtrain = xgb.DMatrix(X_train, label = y1_train)\n","\n","param = { 'verbosity': 0,\n","          'objective': 'reg:pseudohubererror',\n","          'eval_metric': 'mae',\n","          'subsample': 0.8,\n","          'colsample_bytree': 0.8,\n","          'tree_method': 'gpu_hist',\n","          'eta': 0.1,\n","          'max_depth': 5,\n","          'gamma': 0,\n","          'min_child_weight': 1 }\n","\n","bst = xgb.cv(param, dtrain, nfold = 3, num_boost_round = 1000, early_stopping_rounds = 50)"]},{"cell_type":"markdown","metadata":{},"source":["Below is the Mean Absolute Error (MAE) of the training data and validation data during the training."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig = plt.figure(figsize = (12, 4))\n","fig.suptitle('The Mean Absolute Error (MAE) of the training data and validation data')\n","plt.subplot(121)\n","plt.plot(bst['train-mae-mean'], label = 'train')\n","plt.plot(bst['test-mae-mean'], label = 'validation')\n","plt.xlabel('Runs')\n","plt.ylabel('MAE')\n","plt.legend()\n","plt.subplot(122)\n","plt.plot(bst['train-mae-mean'], label = 'train')\n","plt.plot(bst['test-mae-mean'], label = 'validation')\n","plt.yscale('log')\n","plt.xlabel('Runs')\n","plt.ylabel('MAE in log scale')\n","plt.legend()\n","plt.subplots_adjust(wspace = 0.3)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["The following figure shows the importance of each independent variable in building the decision trees in the XGBoost regressor. The most important feature 'f6' is the \"Feature_7\" in the original data. The following three important features 'f25', 'f26', 'f27' are the stock returns in the previous two days and the current day. \n","\n","Since the independent variables are almost not correlated with the target variables, including all of them will introduce many noises in our model. We only use the ten most important independent variables in our model."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["xgb_med = xgb.train(param, dtrain, num_boost_round = bst.shape[0])\n","\n","fig, ax = plt.subplots(figsize = (6, 8))\n","xgb.plot_importance(xgb_med, ax = ax)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Here is an example of one decision tree inside the XGBoost model. It has only one leaf."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots()\n","xgb.plot_tree(xgb_med, ax = ax)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We select the ten most important independent variables."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["select = [1, 5, 6, 13, 14, 18, 22, 25, 26, 27]\n","\n","X_train = X[:30000, select]\n","X_val = X[30000:35000, select]\n","X_test = X[35000:, select]\n","X_train_val = X[:35000, select]"]},{"cell_type":"markdown","metadata":{},"source":["# Model Training with XGBoost Regressor"]},{"cell_type":"markdown","metadata":{},"source":["We carefully train a XGBoost regressor in this section. Notice that we use Pseudo-Huber loss function which is a differentiable approximation of the linear loss function. It is less sensitive to outliers compared to the conventional square loss function. This is a key idea that make our model perform well.\n","\n","We first define a utility function to perform grid search for parameter tuning. The utility function can tune parameters on one-dimensional and two-dimensional grids. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from xgboost import XGBRegressor\n","from sklearn.metrics import mean_absolute_error\n","\n","def mysearch(X_train, y_train, X_val, y_val, param, param1, param2 = None, estimator = XGBRegressor, score = mean_absolute_error):\n","    best_score = 10000000.\n","    best_param = { **param }\n","    para = { **param }\n","    key1 = list(param1.keys())[0]\n","    if param2 is not None:\n","        key2 = list(param2.keys())[0]\n","        for parama in param1[key1]:\n","            for paramb in param2[key2]:\n","                para[key1] = parama\n","                para[key2] = paramb\n","                est = estimator(**para)\n","                est.fit(X_train, y_train)\n","                y_pred = est.predict(X_val)\n","                current_score = score(y_pred, y_val)\n","                #print('The current score: ', current_score)\n","                #print('The current parameter: {} = {}, {} = {}'.format(key1, parama, key2, paramb))\n","                if (current_score < best_score):\n","                    best_score = current_score\n","                    best_param[key1] = parama\n","                    best_param[key2] = paramb\n","        #print('The best score: ', best_score)\n","        #print('The best parameter: {} = {}, {} = {}'.format(key1, best_param[key1], key2, best_param[key2]))\n","    else:\n","        for parama in param1[key1]:\n","            para[key1] = parama\n","            est = estimator(**para)\n","            est.fit(X_train, y_train)\n","            y_pred = est.predict(X_val)\n","            current_score = score(y_pred, y_val)\n","            print('The current score: ', current_score)\n","            print('The current parameter: {} = {}'.format(key1, parama))\n","            if (current_score < best_score):\n","                best_score = current_score\n","                best_param[key1] = parama\n","        #print('The best score: ', best_score)\n","        #print('The best parameter: {} = {}'.format(key1, best_param[key1]))\n","    return best_score, best_param"]},{"cell_type":"markdown","metadata":{},"source":["Here is an example of using the utility function to tune the maximum depth of the tree and the minimum child weight. The tuning of other parameters are omitted here for brevity."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["param = {'learning_rate': 0.1,\n","         'verbosity': 0,\n","         'objective': 'reg:pseudohubererror',\n","         'tree_method': 'gpu_hist',\n","         'n_estimators': 100,\n","         'n_jobs': -1,\n","         'gamma': 0,\n","         'subsample': 0.8,\n","         'colsample_bytree': 0.8,\n","         'alpha': 0}\n","param1 = { 'max_depth': [1, 3, 5] }\n","param2 = { 'min_child_weight': [1, 3, 5] }\n","score, bst_param = mysearch(X_train, y1_train, X_val, y1_val, param, param1, param2)\n","print('The best score is:', score)\n","print('The best parameter is:', bst_param)"]},{"cell_type":"markdown","metadata":{},"source":["Here is the final parameters after all tunings."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["best_xgbr = XGBRegressor( objective = 'reg:pseudohubererror',\n","                          tree_method = 'gpu_hist',\n","                          max_depth = 5,\n","                          min_child_weight = 3,\n","                          gamma = 0,\n","                          subsample = 0.9,\n","                          colsample_bytree = 0.9,\n","                          alpha = 0,\n","                          learning_rate = 0.01,\n","                          n_estimators = 700)"]},{"cell_type":"markdown","metadata":{},"source":["The final models are trained on the unions of the training data and the validation data. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["best_xgbr.fit(X_train_val, y1_train_val)\n","y1_test_pred = best_xgbr.predict(X_test)\n","\n","best_xgbr.fit(X_train_val, y2_train_val)\n","y2_test_pred = best_xgbr.predict(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["We evaluate the model performance on the testing data. The final models consistently perform better than the all-zero prediction."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error\n","\n","benchmark = [np.abs(y1_test).mean(), np.abs(y2_test).mean()]\n","fitted = [mean_absolute_error(y1_test_pred, y_test[:, 0]), mean_absolute_error(y2_test_pred, y_test[:, 1])]\n","\n","index = np.arange(2)\n","bar_width = 0.35\n","plt.bar(index, benchmark, bar_width, label = 'All-zero prediction')\n","plt.bar(index + bar_width, fitted, bar_width, label = 'XGBoost regressor')\n","plt.xticks(index + bar_width / 2, ['Ret_PlusOne', 'Ret_PlusTwo'])\n","plt.title('Comparison of the XGBoost regressor and the all-zero prediction')\n","plt.xlabel('Stock returns in the two following days')\n","plt.ylabel('MAE of the pridiction')\n","plt.ylim([0, 0.021])\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We final train the models on the whole data and make prediction on the unseen data."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["best_xgbr.fit(X[:, select], y[:, 0])\n","y1_new_pred = best_xgbr.predict(new_X[:, select])\n","\n","best_xgbr.fit(X[:, select], y[:, 1])\n","y2_new_pred = best_xgbr.predict(new_X[:, select])"]},{"cell_type":"markdown","metadata":{},"source":["Ensembles of the XGBoost regressor and the median prediction are used for the final results. The median prediction are used here to reduce overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y1_new_pred = 0.1 * y1_new_pred + 0.9 * np.median(y[:, 0])\n","y2_new_pred = 0.1 * y2_new_pred + 0.9 * np.median(y[:, 1])\n","\n","ts_new_pred = np.zeros((new_X.shape[0], 60))\n","\n","zip_file = ZipFile('../input/the-winton-stock-market-challenge/sample_submission_2.csv.zip')\n","sub = pd.read_csv(zip_file.open('sample_submission_2.csv'))\n","sub['Predicted'] = np.hstack([ts_new_pred, y1_new_pred[:, np.newaxis], np.median(y[:, 1])* np.ones((new_X.shape[0], 1))]).flatten()\n","#sub.to_csv('submission.csv', index = False)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":34407,"sourceId":4504,"sourceType":"competition"}],"dockerImageVersionId":30005,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
